Chapter 01
HDF5 is used for storing large numerical arrays of homogenous types for data models that can be organized hierarchically and benefit from tagging of datasets with arbitrary metadata
Good for:
-minimal use of relational features (if relational db is needed use SQL)
-very high performance
-partial I/O, subsetting (basically don't need to load it all into memory)
-hierarchical organization
-arbitrary

HDF5: The File
datasets: array-like objects that store numerical data on disk
groups: hierarchical containers that store datasets and other groups
attributes: user-defined bits of metadata that can be attached to datasets




Chapter 02
h5py
PyTables: a scientific database package based on HDF5 that adds dataset indexing and additional type system

#Required imports
import numpy as np
import h5py

h5py package automatically maps the HDF5 type system onto NumPy dtypes

remember numpy gets back a copy of the array by default, so should make a copy if you are modifing the data
the same logic applies to HDF5 because the data is always read from the disk
when reading HDF5 files, you automatically always make a copy, because the file is read from the disk
the data is stored on the disk

IPython be default will save the output of your statements in special hidden variables. To turn this off cache_size to 0

HDFView #is a program that lets you view you data graphically
ViTables #another program optimized for dealing with PyTables

h5ls -vlr filename.hdf5 #command line tools from www.hdfgroup.org allows you to see the dataset

Creating HDF5 file
f = h5p.File('filename.hdf5')
f.close()

f = h5p.File('filename.hdf5', 'w')
File modes:
'w' new file overwriting any existing file
'r' open read-only (must exist)
'r+' open read-write (must exist)
'a' open read-write (create if it doesn't exist)
'w-' create a new file, but fail if a file of the same name already exists

Good idea to use with statements (similar to C# using statements)
with h5py.File('fileName.txt', 'w') as f:
	f.write(data)
#this will properly close the file

File drivers: set between the file system and the higher-level world of HDF5 groups, datasets and attributes. They deal with the mechanics of mapping HDF5 address space so you don't have to.
Drivers of note:
core driver: stores your file entirely in memory f = h5p.File('filename.hdf5', driver='core')
#HDF5 creates a backing store file that a file image is saved when closed
f = h5p.File('filename.hdf5', driver='core', backing_store=True)

family driver: split a file up into multiple images that all share a certain maximum size
f = h5p.File('filename.hdf5', driver='family', memb_size=1024**3) #2GB file size

mpio driver: the heart of Parallel HDF5

The UserBlock: HDF5 files may be preceeded by arbitrary user data. When the file is open the library looks for the HDF5 header at the beginning of the file, than 512 bytes, and 1024 bytes (powers of 2)
f = h5p.File('filename.hdf5', driver='family', userblock_size=512)
f.userblock_size #prints 512, default is 0



Chapter 03
datasets are the central feature of HDF5, think of them as NumPy arrays that live on disk
data is on the disk not the memory

Every dataset in HDF5 has a:
-name
-type
-shape

HDF can have between 0 and 32 axes

Reading
print(data[...]) #prints out all the data

Dataset basics
f = h5py.File("location.hdf5")
f["my dataset"] = data
dset = f["my dataset"]
#dset is a "proxy" object that lets you read/write the underlying HDF5 dataset to disk
f.close()

Slicing
dset[colStart : colEnd, rowStart : rowEnd]
returns a numpy array

Creating empty datasets
dset = f.create_dataset("name", (size1, size2), dtype=np.complex64, data = myData)
dset = f["my dataset"]
f.close()
out = dset[...]
out[colstart : colEnd, rowStart : rowEnd]

Writing
data[start:end,step] = value
f.flush() #to flush the data to disk
dset is type float32
bigOut is type float64
bigOut = np.empty((100,1000), dtype=np.float64)
dset.read_direct(bigOut) #fills up the empty array with requested data

Better solution
with dset.astype('float64'):
	out = dset[0,:]
out.dtype
example
with h5py.File(r'D:\test2.hdf5','w') as f2:
    f2.create_dataset('big', data=bigdata, dtype=np.float32)

HDF5 library itself handles type conversion and does so on the fly when saving/reading from a file
out_file = np.empty((rows, columns), dtype=np.float64)
#HDF5 fills up the empty array with the requested data
dset.read_direct(out_file)
#simpler solution
with dset.astype('float64):
	out = dset[0, :]

Slicing
Here’s what happens behind the scenes when we do the slicing operation:
1. h5py figures out the shape (10, 50) of the resulting array object.
2. An empty NumPy array is allocated of shape (10, 50).
3. HDF5 selects the appropriate part of the dataset.
4. HDF5 copies data from the dataset into the empty NumPy array.
5. The newly filled in NumPy array is returned.

Take reasonably sized slices
#assuming working with a 100x1000 array
for ix in xrange(100):
	val = dset[ix,:]
	val[val<0] = 0
	dset[ix,:] = val

dset[...] #a stand in for axes you don't bother specifying

Accessing individual element: dset[()]

Coordinate Lists
dset[[pos1, pos2, pos3]]

Automatic Broadcasting
dset[:,:] = dset[0,:] #copying the array

Reading Directly into an Existing Array
dset.read_direct(out, source_sel=np.s_[0,:], dest_sel=np.s_[50,:])
#source_sel ==> source selection
#dest_sel ==> destination selection

little-endian: the least significant byte first (Intel x86)
big-endian: the most significant byte first




Chapter 04
Contiguous storage: all elements of the array are stored one after the other. This is great for reading large parts of the data at once, however if reading certain parts (slicing parts of data), than its not so good
locality: reads are generally faster when the data being accessed is all stored together

Chuncking: splits data into chuncks of the specified shape, flattens them and writes them to disk, this reduces the problems, the chuncks are stored in various places in the file and their coordinates are indexed by a B-tree. Allows you to write the file that "best" fits your data
dset = f.create_dataset('chunked', (100,480,640), dtype=np.int32, chunks=(1,64,64))
dset.chunks #checks if the dataset is in chunks, from our example returns (1, 64, 64)

Auto-chunking
dset = f.create_dataset('chunked', (100,480,640), dtype=np.int32, chunks=True)
the auto-chunker tries to keep chunks mostly "square" (in N dimensions) and within certain size limits

Filters and Compression
WIth chunking it becomes possible to perform compression on a dataset with the initial size of each chunk known and indexed by a B-Tree they can be stored anywhere in the file, each chunk is free to grow/shrink
Filter pipeline: a series of operations performed on each chunk when it's written. When the file is read each filter is run in "reverse" mode to reconstruct the original data
Compression Filters:
GZIP/DEFLATE
SZIP (NASA)
LZF

Can use SHUFFLE with GZIP and LZF for very fast compressions



Chapter 05
Groups and subgroups
groups: are the HDF5 container object, think of them as folders in your OS
File serves as the root group

f = h5py.File(r"D:\Groups.hdf5")
subgroup = f.create_group("SubGroup")
f["Dataset1"] = 1.0
f["Dataset2"] = 2.0
f["Dataset3"] = 3.0
subgroup["Dataset4"] = 4.0
subSubGroup = subgroup.create_group("AnotherGroup")
out = f.create_group('/some/big/path')
f.close()

#this will look like this:
Groups.hdf5
	--> Dataset1
	--> Dataset2
	--> Dataset3
	--> SubGroup
		--> AnotherGroup
		--> Dataset4

Groups work mostly like dictionaries, are iterable, have a subset of the normal Python dictionary API
Dictionary-Style access
dset1 = f["Dataset1"]
dset4 = f["SubGroup/Dataset4"]
if you access a key that doesn't exist it raises an exception
out f.get("BadName") #will not raise an exception if the wrong key is supplied
links: a layer between the group object and the objects that its members
hard links: have an address
soft links: store the path to an object
external links: allow you to refer to objects in other files.
Has 2 components:
1.) the name of a file
2.) the absolute name of an object within that file
using get to determine object types (both keywords)
f.get(data, getclass=True) lets you retrieve the type of an object without actually having to open it
f.get(data, getlink=True) lets you determine the properties of the link involved
When creating a new file and you don't want to overwrite your data, so use the require_dataset or require_group and check for an existing group or dataset and return it instead
f.require_group("SubGroup")
f.require_dataset('Dataset1', (size,), dtype='int64')
Dictionary-style iteration
[x for x in theFile]
[x for x in f.itervalues()] there is also iteritems()
[(x,y) for x, y in f.iteritems()]
iter* methods are, performance speaking much faster/better
visitor iteration: when you want to iterate over every single object in the file. Provide a callable and HDF5 calls it with an argument/2 for every object
visit by name
f.visit(func)
def printname(name):
	print(name)
f.visit(printname) # will print each name of in the dataset
grp.visit(func) #works on groups as well
f.copy('mygroup', 'mygroup2')

OLDER NOTES
f = h5py.File("Groups.hdf5")
subgroup = f.create_group("Subgroup") or out = f.create_group('/some/big/path')
Access
dset = f["Subgroup/Dataset"] #posix-style paths
Objects: have address (byte offset) in the file that HDF5 has to look up
obj.parent #can access parent
this can cause problems with the naming so instead can use the posix path
import posixpath
parent = obj.file[posixpath.dirname(obj.name)]
HDF5 does not free up space on its own, so if space its reused on its own a "hole" might end up in the data. To avoid this use repack:
$ h5repack bigfile.hdf5 out.hdf5
f.get(name, getclass=True, getlink=True) #lets you retrieve type of an object without actually having to open it
Containership testing
if 'name' in group:
	#code
visitor iteration: iterate over every single in the file, or objects "below" a certain group
f.visit('name')



Chapter 06
Attributes: are pieces of metadata you can stick on objects in the file. A key mechanism for making self-describing files.
dset.attrs #to get the attributes of the dataset
can set attributes in a way similar to Python Dicitionaries
dset.attrs['attribute Name'] = value


Chapter 07
Datatypes in HDF5
dset = f.create_dataset('name', (shape), dtype=np.float16)
Fixed-length strings
NumPy uses the "S" dtype
dt = np.dtype("S10) #10 character byte string
dset = f.create_dataset("fixedString", (100,), dtype=dt)
will truncate strings that are too big
Variable-length strings
vlen data type
dt = h5py.special_dtype(vlen=str)
Python 3
vlen=str #UTF-8
vlen=bytes #ASCII
Numpy supports structured arrays
example of a weather station
dt = np.dtype([("temp", np.float), ("pressure", np.float), ("wind", np.float)])
dset = f.create_dataset("name", (shape), dtype=dt)
Enumerated types
example
mapping = {"RED":0, "GREEN":1, "BLUE":2}
dt = h5py.special_dtype(enum=(np.int8, mapping))
dset = f.create_dataset('enum', (100,), dtype=dt)
Opaque types: when data can't be represented in any of the NumPy forms
NumPy void "V" type used to store such "opaque" data


Chapter 08
References, the HDF5 pointer type, store links to objects as data
Named types: enable you to enforce type consistency across datasets
Dimension Scales: (HDF5) standard lets you attach physically meaningful axes to your data in a method that 3rd party programs can understand
dset.ref #the object reference, pointers to objects in the file
dereference object reference
out = f[grp1.ref]
Region references: enable you to store a reference to a part of a dataset. Effectively storing your slicing arguments for dataset access
Dimension scales: HDF5 knows what to do with certain specific combinations of groups, datasets, attributes and references.
Creating Dimension Scales
dset.dims #property of dataset, a separate "axis" dataset with some metadata
Good example of creating dimension scales
To record this, we first create three datasets in the file which will hold our Dimension Scale “axes”:
>>> f.create_dataset('scale_x', data=np.arange(100)*10e3)
>>> f.create_dataset('scale_y', data=np.arange(100)*10e3)
>>> f.create_dataset('scale_z', data=np.arange(100)*100.0)
Now, we ask HDF5 to turn them into official “Dimension Scale” datasets by using the create_scale method on dset.dims:
>>> dset.dims.create_scale(f['scale_x'], "Simulation X (North) axis")
>>> dset.dims.create_scale(f['scale_y'], "Simulation Y (East) axis")
>>> dset.dims.create_scale(f['scale_z'], "Simulation Z (Vertical) axis")
Attaching Scales to a Dataset
>>> dset.dims[0].attach_scale(f['scale_x'])
>>> dset.dims[1].attach_scale(f['scale_y'])
>>> dset.dims[2].attach_scale(f['scale_z'])
Labeling each axis of the dataset
>>> dset.dims[0].label = "x"
>>> dset.dims[1].label = "y"
>>> dset.dims[2].label = "z"


Chapter 09
Parallel Python
Message Passing Interface (MPI)
MPI and Parallel HDF5
Processes are peers
from mpi4py import MPI
comm = MPI.COMM_WORLD
COMM_WORLD object is an MPI communicator
MPI-based HDF5 Program
from mpi4py import MPI
import h5py
f = h5py.File("foo.hdf5", "w", driver="mpio", comm=MPI.COMM_WORLD)

















