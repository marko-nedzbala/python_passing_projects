
Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow

https://colab.research.google.com/github/ageron/handson-ml3/blob/main/

https://github.com/ageron/handson-ml3




Create virtual environment
$ python -m venv .venv
$ source ./.venv/Scripts/Activate.ps1

Install Jupyter Notebook
$ pip install ipykernel
$ pip install notebook

https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-with-commands



Instructions for Docker
https://homl.info/install

Numpy, Pandas, Matplotlib Tutorial
https://homl.info/tutorials

Linear algebra Tutorial
https://homl.info/tutorials

scikit learn User Guide
https://scikit-learn.org/stable/user_guide.html


Errata
https://www.oreilly.com/catalog/errata.csp?isbn=9781098125974&_gl=1*1v6yizz*_ga*MTczNzcxMTQ5Mi4xNzI1MTExMTc5*_ga_4WZYL59WMV*MTc0MzYzNTYzNi4yMjkuMS4xNzQzNjM2NTU0LjM3LjAuMA..




19 Chapters



Preface

Libraries:
Scikit-Learn
TensorFlow
Keras

Part I - "The Fundamentals of Machine Learning"

Chapter01

Machine learning: the art and science of programming computers so they can learn from data.

Some terms
Training set: the examples that the system uses to learn.
Training instance (sample): each training example.
Model: the part of a machine learning system that learns and makes predictions.
Data mining: digging into large amounts of data to discover hidden patterns

Types of Machine Learning Systems
Training Supervision

Supervised learning
The training set you provide to the algorithm includes the desired solutions (labels).
Classification, Regression
Target is more common for regression tasks
Label is more common for classification tasks
Features are sometimes called predictors or attributes

Unsupervised learning
Training set is unlabeled and the system tries to learn without a teacher
Clustering algorithm will attempt to detect groups
Visualization algorithms: provide them data and they produce visual representations
Dimensionality reduction: simplify the data without losing too much information
Feature extraction: extract a feature form a given amount of data
Anomaly detection: detecting unusual activities
Novelty detection: aims to detect new instances that look different from all instances in the training set
Association rule learning: the goal is to dig into large amounts of data and discover interesting relations between attributes

Semi-supervised learning
Labeling data is time-consuming and costly
Partially labeled data
Self-supervised learning: involves generating a fully labeled dataset from a fully unlabeled one.
Transfer learning: transferring knowledge from one task to another

Reinforcement learning
Agent: the learning system, observes the environment, select and perform actions get rewards/penalties
Policy: learn by itself what is the best strategy to get the most reward over time
A policy defines what action the agent should chose when it is in a given situation

Batch learning
The system is incapable of learning incrementally, it must be trained using all available data.
The world continues to evolve around the model and the model remains unchanged resulting in model rot or data drift.

Online learning (incremental learning)
Train the system incrementally by feeding it data instances sequentially, either individually or in small groups called mini-batches
Out-of-core learning: trains models on huge datasets that cannot fit in one machine's main memory
Learning rate: how fast the system should adapt to changing data.

Instance-based learning
The system learns the examples by heart, then generalizes to new cases by using a similarity measure to compare them to the learned examples (or a subset of them).

Model-based learning
Build a model of the examples and use them to make predictions
Utility function (fitness function): that measures how good your model is
Cost function: measures how bad a model is

Challenges of Machine Learning
Insufficient quantity of training data
The idea that data matters more than algorithms for complex problems was further popularized by Peter Norvig et al. in a paper titled “The Unreasonable Effectiveness of Data”, published in 2009

Nonrepresentative training data
Sampling noise: too small sample
Sampling bias: too large sample

Poor-quality data

Irrelevant features
Feature engineering: the process of coming up with a good set of features to train on. Involves the following steps:
1.) Feature selection: selecting the most useful features to train on among existing features
2.) Feature extraction: combining existing features to produce a more useful one
3.) Creating new features by gathering new data

Overfitting the training data
When the model is too complex relative to the amount and noisiness of the training data
Regularization: constraining a model to make it simpler and reduce the risk of overfitting
hyperparameter: the amount of regularization to apply during learning can be controlled here, a parameter of a learning algorithm (not of the model). Not affected by the learning algorithm itself, it must be set prior to training and remains constant during training.

Great explanation
For example, the linear model we defined earlier has two parameters, θ0 and θ1. This gives the learning algorithm two degrees of freedom to adapt the model to the training data: it can tweak both the height (θ0) and the slope (θ1) of the line. If we forced θ1 = 0, the algorithm would have only one degree of freedom and would have a much harder time fitting the data properly: all it could do is move the line up or down to get as close as possible to the training instances, so it would end up around the mean. A very simple model indeed! If we allow the algorithm to modify θ1 but we force it to keep it small, then the learning algorithm will effectively have somewhere in between one and two degrees of freedom. It will produce a model that’s simpler than one with two degrees of freedom, but more complex than one with just one. You want to find the right balance between fitting the training data perfectly and keeping the model simple enough to ensure that it will generalize well.

Underfitting

Testing and Validating
Try out new cases
Split data into 2 sets:
1.) Training set
2.) Test set

Generalization error (Out-of-sample) the error rate on new cases, get this result on the test set and the estimate of this error. A low training error and a high generalization error means you are overfitting the training data.

Holdout validation: simply hold out part of the training set to evaluate several candidate models and select the best one. The new held-out set is called the validation set (development set or dev set).



Chapter02
This is a really good chapter to illustrate the process. Use this as a future reference

Focuses on a real project

The walk through:
1.) Look at the big picture.
2.) Get the data.
3.) Explore and visualize the data to gain insights.
4.) Prepare the data for machine learning algorithms.
5.) Select a model and train it.
6.) Fine-tune your model.
7.) Present your solution.
8.) Launch, monitor, and maintain your system.

Working with Real data
Popular open data repositories:
	OpenML.org
	Kaggle.com
	PapersWithCode.com
	UC Irvine Machine Learning Repository
	Amazon’s AWS datasets
	TensorFlow datasets
Meta portals (they list open data repositories):
	DataPortals.org
	OpenDataMonitor.eu
Other pages listing many popular open data repositories:
	Wikipedia’s list of machine learning datasets
	Quora.com
	The datasets subreddit

1.) Frame the problem

Data pipeline: the sequence of data processing components

Select a performance measure
A typical performance measure for regression problems is the Root Mean Square Error (RMSE)
RMSE(X, h) = SQRT(1/m * sum(h(x^i) - y^i)^2)

Machine learning notations
m: the number of instances in the dataset you are measuring RMSE on
x^i: a vector of all the feature values, excluding the label, of the ith instance in the dataset
y^i: is the label, the desired output value for that instance
X: a matrix containing all the feature values, exlcuding the labels, of all instances in the dataset
h: system's prediction, hypothesis
RMSE(X, h): the cost function measured on the set of examples using your hypothesis h

Mean Absolute Error (MAE)
MAE(X, h) = (1/m)*sum(abs(h*x^i - y^i))

The higher the norm index, the more it focuses on large values and neglects small ones. The RMSE is more sensitive to outliers than the MAE, but when outliers are exponentially rare, the bell-shaped curve, the RMSE performs very well and is generall preferred

2.) Get the data

Colab keyboard shortcuts
Ctrl + M + A insert code above
Ctrl + M + B insert code below
Ctrl + M + H display keyboard shortcuts

Histogram plotting
import matplotlib.pyplot as plt
df.hist(bins=50, figsize=(12, 8))
plt.show()

Create a test set
Data snooping: when you estimate the generalization error using the test set, your estimate will be too optimistice and the system will not perform as well as expected

Generating random test sets
np.random.seed(42) # for our case 42 is probably a reference to Hitchhikers Guide to The Universe
# then call
np.random.permutation()

splitting data
from sklearn.model_selection import train_test_split
train_set, test_set = train_test_split(df, test_size=0.2, random_state=42)

Stratified sampling
df2 = pd.cut(df["column_name"], bins=[0, 1...], labels=[1, 2,...])

from sklearn.model_selection import StratifiedShuffleSplit
splitter = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)
split() yields the training and test indices

Or even simpler
strat_train_set, strat_test_set = train_test_split(df, test_size=0.2, stratify=df["columnName"], random_state=42)

Imputation: when you have missing values you need to set the missing values to some value.
use sklearn.impute import SimpleImputer
imputer = SimpleImuter(strategy="median")

3.) Explore and Visualize the Data to Gain Insights
test data is set aside and only exploring the training set

look for correlations
to compute the standard correlation coefficient (Pearson's r)
df.corr()

scatter matrix() will plot every numerical attribute against every other numerical attribute
from pandas.plotting import scatter_matrix
scatter_matrix(df, figsize=(12, 8))
plt.show()

4.) Prepare the Data for Machine Learning Algorithms

cleaning the data
imputation: set the missing values to some value
SimpleImputer() makes it possible to impute missing values into both the training set and the validation set, the test set and any new data fed to the model
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy="median")

More powerful imputers in the sklearn.impute package
KNNImputer: replaces each missing value with the mean of the k-nearest neighbors' values for that feature, the distance is based on all the available features
IterativeImputer: trains a regression model per feature to predict the missing values based on all the other available features

Design of Scikit-Learn API
Consistency: all objects share a consistent and simple interface
	Estimators: any object that can estimate some parameters based on a dataset, uses the fit()
	Transformers: transform() takes a dataset to transform as a parameter
	Predictors: predict() takes a dataset to transform as a parameter, and score() to measure the quality of the predictions, given a test set
Inspection: all the estimator’s hyperparameters are accessible directly via public instance variables (e.g., imputer.strategy), and all the estimator’s learned parameters are accessible via public instance variables with an underscore suffix (e.g., imputer.statistics_).
Nonproliferation of classes: datasets are represented as NumPy arrays or SciPy sparse matrices, Hyperparameters are just regular Python strings
Composition: reuse of existing blocks
Sensible defaults: provides reasonable default values for most parameters

Handling text and categorical attributes
from sklearn.preprocessing import OrdinalEncode
my_ordinal_encoder = OrdinalEncoder()
my_code = my_ordinal_encoder.fit_transform(column_name)

# get the list of categories
my_ordinal_encoder.categories_

one-hot encoding: only 1 attribute will be equal to 1 (hot) while the others will be 0 (cold). The new attributes are sometimes called dummy attributes

from sklearn.preprocessing import OneHotEncoder
my_hot_encoder = OneHotEncoder()
my_code = my_hot_encoder.fit_transform(column_name)

pandas has a get_dummies()
pd.get_dummies(df)

Feature scaling
machine learning algorithms don't perform well when the input numerical attributes have very different scales

only perform this on the training set

most common way to get attributes to scale:
1.) Min-max scaling (normalization)
2.) Standardization

1.) Min-max scaling (normalization)
for each attribute the values are shifted and rescaled so that they end up ranging from 0 to 1

(value - min) / (min - max)

from sklearn.preprocessing import MinMaxScaler
min_max_scaler = MinMaxScaler(feature_range=(-1, 1))
my_min_max_scaled = min_max_scaler.fit_transform(column_name)

2.) Standardization

(value - mean) / standard_deviation

from sklearn.preprocessing import StandardScaler
std_scaler = StandardScaler()
my_std_scaled = std_scaler.fit_transform(column_name)

if your distribution has a heavy tail, both of the aforementioned methods will not work because they squash most values into a small range. So before scaling the feature, should transform it to shrink the heavy tail and if possible make the distribution roughly symmetrical

Putting the data into buckets
from sklearn.metrics.pairwise import rbf_kernel

Custom transformers

from sklearn.preprocessing import FunctionTransformer
my_transformer = FunctionTransformer(np.log, inverse_func=np.exp)	# inverse function is optional
my_result = my_transformer.transform(df[['column_name']])

if you want your transformer to be trainable, learning some parameters in fit() and using them in transform()
will need to create a custom class, it needs 3 methods:
1.) fit(), must return self
2.) transform()
3.) fit_transform() can get by simply adding TransformerMixin as a base class

Transformation Pipelines
data transformation steps need to be executed in order
Pipeline class to help with the sequences of transformatio

# example pipeline
from sklearn.pipeline import Pipeline
num_pipeline = Pipeline([
    ("impute", SimpleImputer(strategy="median")),
    ("standardize", StandardScaler()),
])

If you don’t want to name the transformers, you can use the make_pipeline() function instead, it takes transformers as positional arguments and creates a Pipeline using the names of the transformers’ classes, in lowercase and without underscores (e.g., "simpleimputer"):
# example
from sklearn.pipeline import make_pipeline
num_pipeline = make_pipeline(SimpleImputer(strategy="median"), StandardScaler())

5.) Select a model and train it
when checking your model, may want to further split the training set into another training set

from sklearn.model_selection import cross_val_score
ree_rmses = -cross_val_score(tree_reg, housing, housing_labels, scoring="neg_root_mean_squared_error", cv=10)

# cv
split the training set into 10 nonoverlapping subsets called folds

utility function: greater is better
cost function: lower is better

ensembles: models composed of many other models

6.) Fine-tune your model

can fiddle with the hyperparameters
GridSearchCV class will search for you


from sklearn.model_selection import GridSearchCV

full_pipeline = Pipeline([
    ("preprocessing", preprocessing),
    ("random_forest", RandomForestRegressor(random_state=42)),
])
param_grid = [
    {'preprocessing__geo__n_clusters': [5, 8, 10],
     'random_forest__max_features': [4, 6, 8]},
    {'preprocessing__geo__n_clusters': [10, 15],
     'random_forest__max_features': [6, 8, 10]},
]
grid_search = GridSearchCV(full_pipeline, param_grid, cv=3, scoring='neg_root_mean_squared_error')
grid_search.fit(housing, housing_labels)

# get the best combinations of parameters
grid_search.best_params_

Randomized Search
similar to GridSearchCV class, will evaluate a fixed number of combinations, selecting a random value for each hyperparameter at every iteration

7.) Present your solution
will share some with step #8

8.) Launch, monitor, and maintain your system
save your model

import joblib
joblib.dump(final_model, "my amazing model.pkl")

can upload it to Google Cloud Storage (GCS)

maintenance is ML Operations (MLOps)



Chapter03
Classification

MNIST dataset 70,000 images of digits handwritten, considered the "Hello World" of Machine Learning





















Part II - "Neural Networks and Deep Learning"


















